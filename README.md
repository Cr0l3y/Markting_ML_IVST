***EN***

# Project

# Marketing Investment Classifier - Machine Learning Project (ENGLISH VERSION)

## üìä Objective
This project aims to classify customers who are likely to join a bank's marketing investment campaign using supervised machine learning algorithms.

---

## üîç Initial Data Analysis

- **Missing Values**: No null values found in the dataset.
- **Data Consistency**: Some inconsistencies found in text format (e.g., uppercase/lowercase).
- **Variable Types**:
  - The target variable `aderencia_investimento` is categorical: **classification problem**.
  - `inadimplencia` and `fez_emprestimo` only have "sim" (yes) and "nao" (no) values.

---

## üìä Exploratory Data Analysis (EDA)

- Distribution of `aderencia_investimento`: 502 "sim", 766 "nao".
- Bar charts for categorical features (`estado_civil`, `escolaridade`, etc.)
- Summary of numerical variables:
  - `idade`: from 19 to 87
  - `saldo`: may contain negative values

---

## üßπ Data Preprocessing

### 1. Separating Features and Target
```python
x = dados.drop("aderencia_investimento", axis=1)
y = dados["aderencia_investimento"]
```

### 2. One-Hot Encoding for Categorical Features
- We used `drop='if_binary'` so that binary columns like `inadimplencia` and `fez_emprestimo` are converted to a single column: 0 for "nao", 1 for "sim".
```python
from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder(drop='if_binary')
x_encoded = encoder.fit_transform(x)
colunas = encoder.get_feature_names_out()
```

### 3. Label Encoding for the Target Variable
```python
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)  # 1 = sim, 0 = nao
```

### 4. Train-Test Split
```python
from sklearn.model_selection import train_test_split
x_treino, x_teste, y_treino, y_teste = train_test_split(x_encoded, y, test_size=0.3, random_state=42)
```

---

## ü§ñ Modeling and Evaluation

### ‚ñ† Dummy Classifier (Baseline)
```python
from sklearn.dummy import DummyClassifier
dummy = DummyClassifier(strategy='most_frequent')
dummy.fit(x_treino, y_treino)
score_dummy = dummy.score(x_teste, y_teste)  # 60.2%
```

### ‚ñ† Decision Tree Classifier
```python
from sklearn.tree import DecisionTreeClassifier
modelo_arvore = DecisionTreeClassifier()
modelo_arvore.fit(x_treino, y_treino)
```
- Full tree: 100% accuracy (overfitting)

#### Limiting Depth to Prevent Overfitting
```python
modelo_podado = DecisionTreeClassifier(max_depth=3)
modelo_podado.fit(x_treino, y_treino)
score_podado = modelo_podado.score(x_teste, y_teste)  # 71.6%
```

### ‚ñ† K-Nearest Neighbors (KNN)

#### Scaling Data to Range 0-1
```python
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
x_treino_scaled = scaler.fit_transform(x_treino.toarray())
x_teste_scaled = scaler.transform(x_teste.toarray())
```

#### Fitting KNN
```python
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(x_treino_scaled, y_treino)
score_knn = knn.score(x_teste_scaled, y_teste)  # 68%
```

---

## üìä Accuracy Comparison

| Model                  | Accuracy |
|------------------------|----------|
| DummyClassifier        | 60.2%    |
| Decision Tree (max=3) | 71.6%    |
| KNN                   | 68%      |

---

## üìÇ Saving Models with Pickle
```python
import pickle
with open("modelo_onehotenc.pkl", "wb") as f:
    pickle.dump(encoder, f)
with open("modelo_arvore.pkl", "wb") as f:
    pickle.dump(modelo_podado, f)
```

---

## üöÄ Summary
- Successfully built our first classification model using decision trees.
- Also implemented KNN to compare performances.
- Best result came from the **Decision Tree with controlled depth**.
- All encodings and models were saved using `pickle` for reuse in future predictions.




---
***PT-BR***

# Projeto

# Classificador de Investimentos em Marketing - Projeto de Aprendizado de M√°quina (VERS√ÉO EM INGL√äS)

## üìä Objetivo
Este projeto visa classificar clientes com probabilidade de participar de uma campanha de investimento em marketing de um banco usando algoritmos supervisionados de aprendizado de m√°quina.
---
## üîç An√°lise Inicial de Dados

- **Valores Ausentes**: Nenhum valor nulo encontrado no conjunto de dados.
- **Consist√™ncia dos Dados**: Algumas inconsist√™ncias encontradas no formato do texto (por exemplo, letras mai√∫sculas/min√∫sculas).
- **Tipos de Vari√°veis**:
    - A vari√°vel alvo `aderencia_investimento` √© categ√≥rica: **problema de classifica√ß√£o**.
    - `inadimplencia` e `fez_emprestimo` s√≥ possuem valores "sim" (sim) e "nao" (n√£o).

---
## üìä An√°lise Explorat√≥ria de Dados (EDA)

- Distribui√ß√£o de `aderencia_investimento`: 502 "sim", 766 "nao".
- Gr√°ficos de barras para caracter√≠sticas categ√≥ricas (`estado_civil`, `escolaridade`, etc.)
- Resumo das vari√°veis ‚Äã‚Äãnum√©ricas:
    - `idade`: de 19 a 87
    - `saldo`: pode conter valores negativos

---
## üßπ Pr√©-processamento de dados

### 1. Separando Recursos e Alvo
```p√≠ton
x = dados.drop("ader√™ncia_investimento", eixo=1)
y = dados["ades√£o_investimento"]
```

### 2. Codifica√ß√£o One-Hot para Recursos Categ√≥ricos
- Usamos `drop='if_binary'` para que colunas bin√°rias como `inadimplencia` e `fez_emprestimo` sejam convertidas em uma √∫nica coluna: 0 para "nao", 1 para "sim".
```python
from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder(drop='if_binary')
x_encoded = encoder.fit_transform(x)
colunas = encoder.get_feature_names_out()
```

### 3. Codifica√ß√£o de r√≥tulo para a vari√°vel de destino
```python
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)  # 1 = sim, 0 = nao
```

### 4. Divis√£o de Treinamento e Teste
```python
from sklearn.model_selection import train_test_split
x_treino, x_teste, y_treino, y_teste = train_test_split(x_encoded, y, test_size=0.3, random_state=42)
```

---

## ü§ñ Modelagem e Avalia√ß√£o

### ‚ñ† Classificador Dummy (Linha de Base)
```python
from sklearn.dummy import DummyClassifier
dummy = DummyClassifier(strategy='most_frequent')
dummy.fit(x_treino, y_treino)
score_dummy = dummy.score(x_teste, y_teste)  # 60.2%
```

### ‚ñ† Classificador de √Årvore de Decis√£o
```python
from sklearn.tree import DecisionTreeClassifier
modelo_arvore = DecisionTreeClassifier()
modelo_arvore.fit(x_treino, y_treino)
```
- √Årvore completa: 100% de precis√£o (overfitting)

#### Limitando a profundidade para evitar overfitting
```python
modelo_podado = DecisionTreeClassifier(max_depth=3)
modelo_podado.fit(x_treino, y_treino)
score_podado = modelo_podado.score(x_teste, y_teste)  # 71.6%
```
### ‚ñ† K-Vizinhos Mais Pr√≥ximos (KNN)

#### Escalando Dados para o Intervalo de 0 a 1
```python
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
x_treino_scaled = scaler.fit_transform(x_treino.toarray())
x_teste_scaled = scaler.transform(x_teste.toarray())
```
#### Adapta√ß√£o KNN
```python
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(x_treino_scaled, y_treino)
score_knn = knn.score(x_teste_scaled, y_teste)  # 68%
```

---

## üìä Compara√ß√£o de Precis√£o

| Model                  | Accuracy |
|------------------------|----------|
| DummyClassifier        | 60.2%    |
| Decision Tree (max=3) | 71.6%    |
| KNN                   | 68%      |

---

## üìÇ Salvando modelos com Pickle
```python
import pickle
with open("modelo_onehotenc.pkl", "wb") as f:
    pickle.dump(encoder, f)
with open("modelo_arvore.pkl", "wb") as f:
    pickle.dump(modelo_podado, f)
```

---
## üöÄ Resumo
- Constru√≠mos com sucesso nosso primeiro modelo de classifica√ß√£o usando √°rvores de decis√£o.
- Tamb√©m implementamos KNN para comparar desempenhos.
- O melhor resultado veio da **√Årvore de Decis√£o com profundidade controlada**.
- Todas as codifica√ß√µes e modelos foram salvos usando `pickle` para reutiliza√ß√£o em previs√µes futuras.